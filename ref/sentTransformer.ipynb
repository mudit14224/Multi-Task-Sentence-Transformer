{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/muditjindal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MultiHead Self Attention Block\n",
    "class SelfMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads): \n",
    "        \"\"\"\n",
    "        d_model: Dimension of embeddings\n",
    "        num_heads: Number of attention heads (must divide d_model evenly)\n",
    "        \"\"\" \n",
    "        super(SelfMultiHeadAttention, self).__init__()\n",
    "        # Check if num_heads divides d_model \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Size per attention head\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Initialize the Q, K and V\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size\n",
    "        batch_size = x.shape[0]\n",
    "        # Pass in the x to Q, K and V and reshape\n",
    "        # Q, K, V: (batch_size, num_heads, seq_length, d_k)\n",
    "        Q = self.q_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        atn_weights = F.softmax(scores, dim=-1) \n",
    "        atn_out = torch.matmul(atn_weights, V) # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        atn_out = atn_out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # Shape: (batch_size, seq_len, d_model)\n",
    "        return self.out_linear(atn_out) # Shape: (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHead Attention\n",
    "attn_layer = SelfMultiHeadAttention(d_model=128, num_heads=8)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128)\n",
    "# Expected (batch_size, seq_len, d_model)\n",
    "print(attn_layer(test_input).shape)\n",
    "assert test_input.shape == attn_layer(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test FeedForward\n",
    "ff_layer = FeedForward(d_model=128, d_ff=512)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128)\n",
    "# Expected: (batch_size, seq_len, d_model)\n",
    "print(ff_layer(test_input).shape)\n",
    "assert test_input.shape == ff_layer(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (sin & cosine curves)\n",
    "class PositionalEncode(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncode, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Encode even indices (sin curve)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Encode the odd indices (cos curve)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_len, d_model) (to handle with batch_size)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test PositionalEncoding\n",
    "pos_enc = PositionalEncode(d_model=128, max_len=50)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128) \n",
    "# Expected: (batch_size, seq_len, d_model)\n",
    "print(pos_enc(test_input).shape)\n",
    "assert test_input.shape == pos_enc(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder class\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.2):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_atn = SelfMultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atn_out = self.self_atn(x)\n",
    "        x = self.norm1(x + self.dropout(atn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test TransformerEncoderLayer\n",
    "encoder_layer = TransformerEncoder(d_model=128, num_heads=8, d_ff=512)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128)\n",
    "# Expected: (batch_size, seq_len, d_model)\n",
    "print(encoder_layer(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer class\n",
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=100):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encode = PositionalEncode(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encode(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        x = x.permute(0, 2, 1) # Shape: (batch_size, d_model, seq_len)\n",
    "        x = self.pooling(x).squeeze(-1) # Shape (batch_size, d_model, 1) => (batch_size, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test SentenceTransformer\n",
    "VOCAB_SIZE = 10000\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = 50\n",
    "\n",
    "model = SentenceTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, MAX_LEN)\n",
    "# (batch_size, seq_len)\n",
    "test_input = torch.randint(0, VOCAB_SIZE, (2, MAX_LEN))\n",
    "# Expected: (batch_size, d_model)\n",
    "print(model(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to preprocess sentences. Gets vocab and the padded sentence\n",
    "def preprocess_sentences(sentences):\n",
    "    # Create a vocab\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    vocab['<PAD>'] = 0\n",
    "\n",
    "    # Tokenize and convert to IDs\n",
    "    tokenized = [word_tokenize(s.lower()) for s in sentences]\n",
    "    print(tokenized)\n",
    "    indexed = [[vocab[w] for w in s] for s in tokenized]\n",
    "    print(indexed)\n",
    "\n",
    "    # Padding\n",
    "    max_len = max(len(s) for s in indexed)\n",
    "    # Add padding to shorter sentences\n",
    "    padded = [s + [0] * (max_len - len(s)) for s in indexed] \n",
    "\n",
    "    # Convert to tensor\n",
    "    padded_tensor = torch.tensor(padded)\n",
    "\n",
    "    return vocab, padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'cat', 'sat', 'on', 'the', 'mat', '.'], ['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'], ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', '.']]\n",
      "[[1, 2, 3, 4, 1, 5, 6], [7, 8, 9, 10, 11, 12, 1, 13, 14, 6], [15, 16, 17, 18, 1, 19, 6]]\n"
     ]
    }
   ],
   "source": [
    "vocab, padded_tensor = preprocess_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  1,  5,  6,  0,  0,  0],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1, 13, 14,  6],\n",
       "        [15, 16, 17, 18,  1, 19,  6,  0,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.preprocess_sentences.<locals>.<lambda>()>,\n",
       "            {'<PAD>': 0,\n",
       "             'the': 1,\n",
       "             'cat': 2,\n",
       "             'sat': 3,\n",
       "             'on': 4,\n",
       "             'mat': 5,\n",
       "             '.': 6,\n",
       "             'a': 7,\n",
       "             'quick': 8,\n",
       "             'brown': 9,\n",
       "             'fox': 10,\n",
       "             'jumps': 11,\n",
       "             'over': 12,\n",
       "             'lazy': 13,\n",
       "             'dog': 14,\n",
       "             'artificial': 15,\n",
       "             'intelligence': 16,\n",
       "             'is': 17,\n",
       "             'transforming': 18,\n",
       "             'world': 19})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tensor.shape # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 128])\n",
      "Embeddings: tensor([[-3.6997e-01,  9.9378e-02,  7.7293e-02, -7.0288e-01,  2.6350e-01,\n",
      "          4.0868e-01, -9.3441e-01,  1.3771e-01,  2.7946e-01, -1.1658e+00,\n",
      "          1.8920e-01, -7.4656e-01, -1.7652e-01, -1.6289e-01,  3.4735e-01,\n",
      "          2.4659e-02, -1.3360e-01,  3.5636e-01,  4.2277e-01, -1.7610e-01,\n",
      "          1.1625e-01,  3.4235e-01, -2.7329e-01,  2.7175e-01,  1.0855e-02,\n",
      "          1.5494e+00,  1.4999e-02, -9.9687e-01, -9.1255e-01, -1.9692e-01,\n",
      "         -1.3214e+00,  8.6913e-01, -4.6526e-01,  1.5195e+00, -4.5031e-01,\n",
      "          1.3111e+00, -5.4508e-01,  8.3839e-01,  4.6041e-01,  1.2661e-01,\n",
      "          1.5603e-01, -2.6557e-02, -7.2842e-01, -4.6231e-01, -4.7151e-01,\n",
      "          8.1615e-02, -1.0664e-01,  1.9503e-01, -1.2073e+00, -4.4065e-01,\n",
      "          5.3624e-01,  3.8824e-01,  1.9127e-01, -7.3634e-01, -9.9791e-01,\n",
      "          1.4601e+00,  1.3439e-02,  9.7795e-01,  3.2617e-01,  5.5319e-01,\n",
      "         -4.2673e-01,  1.2521e+00,  4.0913e-02, -1.7105e-01, -1.3210e-01,\n",
      "          2.4894e-02, -1.5621e+00,  2.5049e-01, -1.3873e+00,  6.3166e-01,\n",
      "          5.7597e-02,  5.4200e-01, -7.4200e-01,  1.0761e+00,  3.0369e-01,\n",
      "          2.6093e-01, -7.4191e-01,  1.7886e+00, -1.5445e+00,  8.9517e-02,\n",
      "         -8.8827e-01,  3.2790e-01, -2.8582e-01,  1.9786e-01,  2.8168e-01,\n",
      "          1.6556e-01,  1.9489e-01, -1.2547e-01, -1.0823e+00,  9.8041e-01,\n",
      "         -3.7058e-01,  7.9266e-02, -7.9560e-01,  5.7346e-01, -1.9598e+00,\n",
      "         -5.2815e-01, -3.0113e-01, -1.6887e-01,  6.7672e-01, -2.4152e-01,\n",
      "          6.7145e-02,  1.8842e-02, -9.6465e-02, -5.1463e-01, -7.2738e-02,\n",
      "         -4.2836e-01, -7.3147e-02,  5.2567e-01, -7.7357e-01,  6.8791e-01,\n",
      "         -1.3016e-01,  6.4708e-02, -2.5307e-01,  1.7216e-01,  3.4699e-02,\n",
      "          1.0159e+00, -1.6753e-01,  1.8617e-01, -2.7539e-01,  6.0221e-01,\n",
      "          3.1387e-01,  6.5680e-01,  1.8867e-01, -2.5001e-01,  1.3143e-01,\n",
      "          6.9268e-01,  7.5363e-01,  5.7317e-01],\n",
      "        [-1.5069e+00, -6.0873e-01,  7.5104e-02, -6.4248e-01, -2.4710e-01,\n",
      "          4.3718e-01, -2.3056e-01, -3.4425e-01,  5.8732e-02, -1.2363e+00,\n",
      "          6.4380e-01, -1.3391e+00,  2.0551e-02, -1.5522e-01,  4.5881e-01,\n",
      "          2.5784e-01,  2.4555e-01,  2.8855e-01,  7.7931e-01, -1.4091e-01,\n",
      "          2.6406e-01,  2.8976e-01, -3.5594e-01,  3.7827e-01,  4.1327e-01,\n",
      "          1.1597e+00,  2.1366e-01, -8.7901e-01, -8.4939e-01,  2.1160e-01,\n",
      "         -9.3499e-02,  1.3150e+00, -6.9264e-01,  6.2274e-01, -5.3960e-01,\n",
      "          4.6054e-01, -6.3694e-01,  9.8453e-01,  3.6730e-01,  2.5169e-01,\n",
      "         -3.5730e-01, -1.5886e-01,  4.0456e-01, -4.6773e-01, -7.6011e-01,\n",
      "         -1.4286e-01, -1.4476e-01,  3.9919e-01, -3.3663e-01,  4.5740e-01,\n",
      "          4.4198e-01,  1.0916e+00, -3.0508e-01, -2.4990e-01, -3.9514e-01,\n",
      "          1.0421e+00, -4.1129e-01,  8.3943e-01, -1.6312e-01, -2.1834e-01,\n",
      "         -4.2537e-01,  8.7663e-01,  4.4347e-02,  4.6664e-02, -4.1835e-01,\n",
      "          6.5234e-01, -6.3166e-01,  7.1812e-01, -1.1246e+00,  5.9631e-01,\n",
      "          4.7334e-02,  6.8194e-01, -5.1187e-01, -2.5226e-01, -2.9272e-02,\n",
      "          5.4148e-01, -9.7682e-01,  1.3538e+00, -9.7378e-01,  6.6853e-01,\n",
      "         -9.4031e-01, -3.8180e-01, -3.1240e-01, -2.5693e-01, -1.7690e-01,\n",
      "          3.3996e-01, -5.0842e-01,  4.7114e-01, -5.8100e-01,  7.6331e-01,\n",
      "          3.0283e-01,  2.9277e-01, -5.4692e-01,  2.9462e-02, -9.5773e-01,\n",
      "          1.8895e-01, -1.8616e-01, -1.0855e-01,  4.2034e-01, -2.8140e-01,\n",
      "         -3.6124e-01,  2.5365e-01, -6.7247e-01, -1.1859e-01, -3.0748e-01,\n",
      "          2.3667e-01,  2.0678e-02, -3.4709e-02, -6.1310e-01,  1.7950e-01,\n",
      "         -3.7053e-01, -3.3682e-01, -5.7076e-01,  1.0739e+00, -4.8188e-02,\n",
      "         -2.6134e-02, -6.7445e-02,  2.2483e-01, -8.6870e-01,  5.4852e-01,\n",
      "          1.1024e-01,  7.9192e-01,  3.0698e-02,  1.0042e-01,  5.5683e-02,\n",
      "          8.1207e-01,  8.2866e-01,  4.1088e-01],\n",
      "        [-7.2960e-01, -1.6810e-01,  2.7118e-02, -3.8835e-01, -6.9492e-02,\n",
      "          4.0988e-01, -9.9800e-01, -1.3468e-01, -1.7816e-01, -8.1225e-01,\n",
      "          1.1181e-01, -7.8787e-01,  1.7356e-01, -1.3037e-02,  3.2248e-01,\n",
      "          2.8065e-01, -3.7825e-01, -9.0866e-03, -1.3466e-01,  3.9804e-01,\n",
      "          3.4375e-01,  3.0135e-01, -6.4157e-01,  5.4156e-01, -4.3996e-01,\n",
      "          1.5243e+00, -7.8212e-02, -9.9411e-01, -9.2813e-01, -3.3025e-01,\n",
      "         -1.0361e+00,  7.3769e-01, -1.8746e-01,  1.3846e+00, -5.7269e-01,\n",
      "          1.3149e+00, -7.4580e-01,  9.9275e-01,  5.3247e-01,  6.2846e-01,\n",
      "         -3.0888e-02,  1.5647e-01, -9.4185e-01, -1.9684e-01, -6.3304e-01,\n",
      "          9.3657e-02, -6.8506e-01, -2.1553e-03, -1.2537e+00,  2.5837e-01,\n",
      "          5.3669e-01,  5.3876e-01, -9.0894e-02, -2.6085e-01, -1.1216e+00,\n",
      "          1.0660e+00,  1.8036e-01,  8.1059e-01,  8.2474e-04,  4.1320e-01,\n",
      "         -4.6251e-01,  1.7209e+00, -2.1812e-01, -1.4368e-01, -4.4129e-01,\n",
      "          5.7822e-01, -1.8562e+00,  5.1761e-01, -8.3487e-01,  6.9358e-01,\n",
      "         -1.8721e-01,  1.0808e+00, -3.5625e-01,  2.8131e-01, -3.0200e-02,\n",
      "          2.2151e-01, -1.0216e+00,  1.3977e+00, -1.4096e+00,  2.7179e-01,\n",
      "         -5.0887e-01,  8.9274e-01, -1.6660e-01,  4.6800e-01,  5.2211e-01,\n",
      "         -2.9902e-01,  5.4311e-01,  2.6069e-01, -1.0680e+00,  7.5375e-01,\n",
      "         -4.2617e-01, -3.8007e-02, -7.7873e-01,  7.2097e-01, -1.2855e+00,\n",
      "         -3.8157e-01, -5.4747e-02, -2.5196e-01,  1.5973e-01,  6.1834e-02,\n",
      "         -2.7483e-01,  1.3768e-01,  1.7307e-01, -3.6812e-01, -3.3965e-01,\n",
      "          5.6680e-01, -4.5320e-01,  7.3195e-01, -6.2838e-01,  2.6578e-01,\n",
      "         -5.6924e-02, -7.1551e-02,  3.3050e-01,  9.2501e-01, -4.0090e-01,\n",
      "          5.6020e-01, -1.2616e-01,  5.2086e-01, -4.6585e-01,  4.1361e-01,\n",
      "          4.1387e-01,  1.8199e-01,  3.2077e-01, -4.5639e-01, -8.2988e-01,\n",
      "          7.4071e-01,  4.5928e-01,  6.9637e-01]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128 # Dimension of the fixed size sentence embedding\n",
    "NUM_HEADS = 8 # Number of heads for multi head attention \n",
    "NUM_LAYERS = 4 # Number of repeated multiheadattention blocks\n",
    "D_FF = 512 # intermediate dimension for feedforward network \n",
    "model = SentenceTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, max_len=len(padded_tensor[0]))\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = model(padded_tensor)\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # Expected shape: (batch_size, D_MODEL)\n",
    "print(\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Multi-Task Learning Expansion\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, num_classes_a, num_classes_b, max_len=100):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encode = PositionalEncode(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Task A: Sentence Classification Head\n",
    "        self.classifier_a = nn.Linear(d_model, num_classes_a)\n",
    "\n",
    "        # Task B: Named Entity Recognition (NER) - per token basis Head\n",
    "        self.ner_classifier_b = nn.Linear(d_model, num_classes_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> Shape: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encode(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Task A: Sentence Classification\n",
    "        # we want to pool over the seq_len \n",
    "        x_pooled = x.permute(0, 2, 1) # Shape: (batch_size, d_model, seq_len)\n",
    "        x_pooled = self.pooling(x_pooled).squeeze(-1) # Shape: (batch_size, d_model)\n",
    "        out_task_a = self.classifier_a(x_pooled) # Shape: (batch_size, num_classes_a)\n",
    "        # Task B: NER \n",
    "        # We pass in x and not x_pooled as we want per token label in NER\n",
    "        out_task_b = self.ner_classifier_b(x) # Shape: (batch_size, seq_len, num_classes_b)\n",
    "\n",
    "        return out_task_a, out_task_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification: torch.Size([2, 3])\n",
      "NER: torch.Size([2, 50, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test Multi Task Transformer\n",
    "# Model Initialization\n",
    "VOCAB_SIZE = 10000\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = 50\n",
    "NUM_CLASSES_A = 3 \n",
    "NUM_CLASSES_B = 4  \n",
    "\n",
    "multi_task_model = MultiTaskTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, NUM_CLASSES_A, NUM_CLASSES_B, MAX_LEN)\n",
    "\n",
    "test_input = torch.randint(0, VOCAB_SIZE, (2, MAX_LEN)) # (batch_size, seq_len)\n",
    "out_task_a, out_task_b = multi_task_model(test_input)\n",
    "print(f\"Sentence Classification: {out_task_a.shape}\") # Expected: (batch_size, num_classes_a)\n",
    "print(f\"NER: {out_task_b.shape}\") # Expected: (batch_size, seq_len, num_classes_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification output: tensor([[ 0.0507,  0.0416,  0.4814],\n",
      "        [ 0.0211, -0.0633,  0.4029]], grad_fn=<AddmmBackward0>)\n",
      "NER output: tensor([[[ 1.1841e+00, -7.1495e-01,  4.8525e-02,  4.7646e-02],\n",
      "         [ 5.4922e-01,  9.4005e-02,  2.2591e-01, -6.1908e-02],\n",
      "         [ 1.4539e-01,  1.2950e+00,  3.8574e-01, -5.3482e-02],\n",
      "         [ 8.1093e-01,  4.4390e-01, -4.9584e-01, -2.8783e-01],\n",
      "         [ 7.4695e-01,  1.0204e+00,  3.4611e-01,  6.7273e-01],\n",
      "         [ 1.3796e-01,  9.7461e-01,  2.8913e-01,  7.6818e-01],\n",
      "         [ 1.0523e+00,  1.0082e-01, -5.5909e-01, -9.7521e-01],\n",
      "         [ 7.8043e-01,  2.6486e-02,  2.0210e-02, -3.7988e-01],\n",
      "         [ 7.7239e-01, -8.7847e-01, -5.0499e-01,  9.2409e-01],\n",
      "         [ 3.5308e-01, -1.0130e-01,  3.8011e-01,  9.8022e-02],\n",
      "         [ 4.5162e-01,  7.6195e-01,  8.0909e-01,  1.0101e+00],\n",
      "         [ 9.8108e-01, -2.8731e-01, -5.2479e-01,  3.0944e-01],\n",
      "         [-2.7185e-01, -3.7539e-01,  1.2449e-02, -1.1229e-01],\n",
      "         [ 7.6041e-01,  4.6045e-01, -1.3892e-02,  1.4374e-01],\n",
      "         [ 5.3881e-01,  3.9088e-01,  4.2117e-01, -2.7233e-01],\n",
      "         [ 5.7802e-02,  6.8310e-01, -6.0630e-02, -3.5230e-01],\n",
      "         [ 1.0521e-02,  1.9117e-01, -3.0387e-01, -2.3462e-01],\n",
      "         [-6.9883e-02, -5.2092e-01,  3.3382e-01,  6.5678e-02],\n",
      "         [-7.8900e-01,  6.0496e-01,  4.4762e-01,  8.2207e-01],\n",
      "         [-2.9758e-01,  6.6629e-01,  6.0451e-01,  1.0662e-01],\n",
      "         [ 2.4023e-01,  3.1528e-02,  8.2318e-01,  5.0933e-01],\n",
      "         [ 3.6388e-01,  1.6430e+00,  1.0145e+00, -5.7986e-02],\n",
      "         [ 2.6261e-01,  2.8037e-02, -4.5572e-01, -2.9414e-01],\n",
      "         [ 1.2812e-01,  1.2146e+00,  5.2950e-01, -1.3664e-01],\n",
      "         [ 8.3233e-01,  4.2575e-01,  3.7748e-01, -1.3812e-02],\n",
      "         [-4.2596e-02,  2.8478e-01,  1.3600e-01,  2.9650e-01],\n",
      "         [ 8.4366e-01,  1.9090e-01, -3.1710e-01, -8.3881e-02],\n",
      "         [ 9.3871e-01,  7.1929e-02,  4.1072e-01,  7.3170e-01],\n",
      "         [ 5.8550e-01,  8.2547e-01, -4.0498e-01,  6.2057e-01],\n",
      "         [ 1.1647e-01,  1.0156e+00,  2.3019e-01, -1.3383e-01],\n",
      "         [ 5.1034e-01,  2.2492e+00, -1.3907e-01,  4.4347e-01],\n",
      "         [ 4.4980e-01,  7.2128e-01, -5.6424e-01,  4.6896e-01],\n",
      "         [ 4.2983e-01, -2.5312e-01,  6.6905e-01,  5.0738e-01],\n",
      "         [ 1.3312e-02,  1.0046e+00, -3.0641e-01,  5.2246e-01],\n",
      "         [-2.7870e-01,  6.1286e-01,  2.1703e-02, -1.7973e-01],\n",
      "         [-3.6230e-01, -2.5244e-01, -3.8707e-01, -3.9000e-01],\n",
      "         [ 6.3760e-01,  6.7363e-01,  2.0863e-01, -8.8130e-02],\n",
      "         [-9.0497e-02,  1.0844e+00,  2.7966e-01,  6.8694e-01],\n",
      "         [-1.2089e-01,  1.1811e+00,  1.5141e-01,  1.5297e-01],\n",
      "         [ 8.4713e-01,  4.0847e-01,  4.8109e-01,  4.8080e-01],\n",
      "         [-2.7033e-02, -6.3139e-03,  8.6737e-01,  9.0017e-01],\n",
      "         [-1.4573e-01,  5.1426e-01,  5.6034e-01,  9.9722e-01],\n",
      "         [ 4.5725e-01,  5.7987e-01,  6.7792e-01, -1.6634e-01],\n",
      "         [ 1.6899e-01,  4.4586e-01,  1.7748e-01,  6.1099e-02],\n",
      "         [-3.7743e-01, -4.0622e-01,  6.4149e-02,  4.0411e-01],\n",
      "         [ 1.1978e+00, -1.4953e-01, -5.7474e-01,  1.0957e+00],\n",
      "         [ 4.4598e-01,  1.9849e-01, -5.3644e-06,  1.3400e-01],\n",
      "         [ 8.2871e-02,  5.0151e-01,  4.4030e-01,  7.0451e-02],\n",
      "         [ 5.4754e-01, -6.6721e-01,  1.1982e+00, -1.3828e-01],\n",
      "         [ 7.5834e-02, -1.9557e-01, -2.3254e-02, -1.5362e-01]],\n",
      "\n",
      "        [[ 4.9595e-02,  1.5446e-01, -1.1245e-01,  5.9561e-01],\n",
      "         [ 3.1158e-01, -7.2198e-02,  4.7828e-01,  4.8149e-01],\n",
      "         [ 1.7861e-01,  5.4629e-01,  3.2219e-01,  4.6050e-01],\n",
      "         [ 4.6165e-01, -1.9819e-01,  1.3421e-01, -1.4430e-01],\n",
      "         [ 8.6107e-01,  3.3163e-01,  2.4053e-01,  1.2688e-01],\n",
      "         [ 8.4856e-01, -1.3552e-01,  8.2880e-02,  7.8671e-01],\n",
      "         [ 7.2709e-01, -4.6602e-01,  9.4963e-01, -2.7810e-02],\n",
      "         [ 3.5175e-01,  1.7567e-01,  9.0398e-01,  5.3608e-01],\n",
      "         [-3.1468e-01,  3.4698e-01,  3.0299e-01, -1.6419e-01],\n",
      "         [ 1.4799e-01, -1.8122e-01,  3.7738e-01, -6.2392e-01],\n",
      "         [ 8.2874e-01,  6.2549e-01, -5.5613e-01, -1.0114e+00],\n",
      "         [ 6.4047e-01,  5.5320e-01, -2.6811e-01,  5.0416e-01],\n",
      "         [ 1.1274e-01,  3.8513e-01, -3.9971e-01, -3.5849e-02],\n",
      "         [ 2.0716e-01,  1.8157e-01, -3.5674e-01, -7.0010e-01],\n",
      "         [ 5.1312e-01,  3.4830e-01, -6.2940e-02,  1.1139e-01],\n",
      "         [ 1.0763e+00,  4.9311e-01, -2.0243e-01, -4.0011e-02],\n",
      "         [-3.3585e-01,  9.7947e-01, -2.8073e-01, -4.4370e-01],\n",
      "         [ 3.7423e-01,  6.6155e-01,  1.1097e-01, -5.3813e-01],\n",
      "         [ 2.5627e-01,  1.3182e+00,  3.9846e-01,  1.7115e-01],\n",
      "         [-1.5176e-01, -2.8728e-01,  5.1271e-01,  1.1445e-01],\n",
      "         [-4.8801e-02,  1.7205e+00, -2.9861e-01,  5.1711e-01],\n",
      "         [-5.9306e-01,  2.3329e-01, -1.8198e-01,  4.2072e-01],\n",
      "         [ 6.0225e-02,  4.5432e-01,  1.5762e-01, -4.9481e-01],\n",
      "         [-8.5678e-01,  1.0135e+00,  4.2218e-01,  4.3087e-01],\n",
      "         [ 4.8423e-01,  5.4769e-01,  8.5824e-01, -1.7488e-01],\n",
      "         [-2.2419e-02,  1.5412e-02,  9.3650e-01,  1.2412e-01],\n",
      "         [ 5.6594e-01,  1.1264e+00, -6.5943e-01, -2.9143e-01],\n",
      "         [ 6.6800e-01,  6.0601e-01, -1.1571e-01,  1.7338e-01],\n",
      "         [-7.1831e-01,  8.6909e-01,  4.2670e-01,  3.4464e-01],\n",
      "         [ 8.1612e-01,  2.0652e-01,  9.3815e-01, -3.0806e-01],\n",
      "         [ 7.4690e-01,  5.4821e-01, -1.2237e-01, -3.7405e-01],\n",
      "         [ 3.3735e-01,  7.3466e-01, -4.1026e-01, -5.3356e-01],\n",
      "         [-8.2013e-02,  5.4668e-01, -2.2229e-01,  3.6112e-02],\n",
      "         [-9.9036e-01,  2.8677e-01, -7.6131e-01,  1.8955e-01],\n",
      "         [ 3.5483e-01,  1.3599e-01, -2.4828e-01,  2.2702e-01],\n",
      "         [ 1.2561e+00,  7.7104e-01, -6.8102e-01, -2.5955e-01],\n",
      "         [-5.8845e-01, -3.5706e-01, -3.1377e-01, -8.8519e-01],\n",
      "         [ 1.6949e-01,  9.3599e-01,  2.2946e-01, -1.6488e-01],\n",
      "         [ 6.0217e-01,  5.5441e-02,  5.8120e-01,  2.2534e-01],\n",
      "         [-1.0707e-01, -2.4684e-01, -1.9244e-01,  1.0821e+00],\n",
      "         [ 4.3644e-01, -3.0220e-01,  2.5132e-01,  2.3537e-01],\n",
      "         [ 4.4176e-01, -1.6268e-01,  8.8757e-01,  2.5536e-01],\n",
      "         [-1.6193e-01, -9.6262e-02,  1.0195e+00,  2.6081e-01],\n",
      "         [-6.1400e-01,  7.3599e-01,  8.0867e-01,  1.8800e-01],\n",
      "         [ 3.7255e-02,  7.6188e-01,  3.8255e-01,  9.0302e-01],\n",
      "         [ 3.3191e-01, -1.6765e-01,  6.4591e-01,  3.5720e-01],\n",
      "         [-4.5915e-01,  1.0182e+00, -8.5854e-01,  9.7018e-01],\n",
      "         [-4.9604e-01,  6.4798e-02, -9.4369e-01,  2.4456e-01],\n",
      "         [ 1.9346e-01,  5.0406e-01, -3.1101e-01, -2.4326e-01],\n",
      "         [ 3.8264e-02, -4.7004e-01, -3.6493e-01,  2.0324e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence Classification output: {out_task_a}\")\n",
    "print(f\"NER output: {out_task_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification Prediction: [2, 2]\n",
      "NER Prediction: [[0, 0, 1, 0, 1, 1, 0, 0, 3, 2, 3, 0, 2, 0, 0, 1, 1, 2, 3, 1, 2, 1, 0, 1, 0, 3, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 3, 3, 2, 1, 3, 0, 0, 1, 2, 0], [3, 3, 1, 0, 0, 0, 2, 2, 1, 2, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 1, 3, 1, 1, 2, 2, 1, 0, 1, 2, 0, 1, 1, 1, 0, 0, 2, 1, 0, 3, 0, 2, 2, 2, 3, 2, 1, 3, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Classification Prediction:\", torch.argmax(out_task_a, dim=1).tolist())\n",
    "print(\"NER Prediction:\", torch.argmax(out_task_b, dim=2).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barack', 'obama', 'was', 'the', '44th', 'president', 'of', 'the', 'united', 'states', '.'], ['apple', 'inc.', 'is', 'based', 'in', 'cupertino', ',', 'california', '.']]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 4, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 10]]\n",
      "Sentence Classification: torch.Size([2, 3])\n",
      "NER: torch.Size([2, 11, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test out on sentences\n",
    "sentences = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"Apple Inc. is based in Cupertino, California.\"\n",
    "]\n",
    "\n",
    "vocab, padded_tensor = preprocess_sentences(sentences)\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = len(padded_tensor[0])\n",
    "NUM_CLASSES_A = 3 \n",
    "NUM_CLASSES_B = 4  \n",
    "\n",
    "multi_task_model = MultiTaskTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, NUM_CLASSES_A, NUM_CLASSES_B, MAX_LEN)\n",
    "out_task_a, out_task_b = multi_task_model(padded_tensor)\n",
    "print(f\"Sentence Classification: {out_task_a.shape}\") # Expected: (batch_size, num_classes_a)\n",
    "print(f\"NER: {out_task_b.shape}\") # Expected: (batch_size, seq_len, num_classes_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification Prediction: [0, 1]\n",
      "NER Prediction: [[2, 2, 3, 2, 1, 3, 3, 2, 1, 2, 1], [0, 0, 3, 1, 0, 3, 0, 1, 1, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Classification Prediction:\", torch.argmax(out_task_a, dim=1).tolist())\n",
    "print(\"NER Prediction:\", torch.argmax(out_task_b, dim=2).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop function\n",
    "def train_model(model, optimizer, lf_a, lf_b, inputs, labels_a, labels_b, num_classes_b, epochs=3):\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the outputs from the model\n",
    "        out_task_a, out_task_b = model(inputs)\n",
    "\n",
    "        # Calculate the loss for both the tasks\n",
    "        loss_a = lf_a(out_task_a, labels_a)\n",
    "        loss_b = lf_b(out_task_b.view(-1, num_classes_b), labels_b.view(-1))\n",
    "        \n",
    "        # Get the total loss\n",
    "        total_loss = loss_a + loss_b\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss Task A: {loss_a.item():.4f}, Loss Task B: {loss_b.item():.4f}, Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barack', 'obama', 'was', 'the', '44th', 'president', 'of', 'the', 'united', 'states', '.'], ['apple', 'inc.', 'is', 'based', 'in', 'cupertino', ',', 'california', '.'], ['the', 'eiffel', 'tower', 'is', 'located', 'in', 'paris', '.'], ['elon', 'musk', 'is', 'the', 'ceo', 'of', 'tesla', '.'], ['microsoft', 'corporation', 'is', 'headquartered', 'in', 'redmond', '.'], ['google', 'was', 'founded', 'by', 'larry', 'page', 'and', 'sergey', 'brin', '.'], ['the', 'great', 'wall', 'of', 'china', 'is', 'a', 'famous', 'landmark', '.']]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 4, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 10], [4, 19, 20, 13, 21, 15, 22, 10], [23, 24, 13, 4, 25, 7, 26, 10], [27, 28, 13, 29, 15, 30, 10], [31, 3, 32, 33, 34, 35, 36, 37, 38, 10], [4, 39, 40, 7, 41, 13, 42, 43, 44, 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:00<00:00, 79.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss Task A: 1.1526, Loss Task B: 1.4956, Total Loss: 2.6481\n",
      "Epoch 2: Loss Task A: 1.1794, Loss Task B: 1.4776, Total Loss: 2.6571\n",
      "Epoch 3: Loss Task A: 1.1631, Loss Task B: 1.4883, Total Loss: 2.6513\n",
      "Epoch 4: Loss Task A: 1.1579, Loss Task B: 1.4805, Total Loss: 2.6385\n",
      "Epoch 5: Loss Task A: 1.1877, Loss Task B: 1.4545, Total Loss: 2.6422\n",
      "Epoch 6: Loss Task A: 1.1773, Loss Task B: 1.4739, Total Loss: 2.6512\n",
      "Epoch 7: Loss Task A: 1.1503, Loss Task B: 1.4629, Total Loss: 2.6132\n",
      "Epoch 8: Loss Task A: 1.2007, Loss Task B: 1.4954, Total Loss: 2.6961\n",
      "Epoch 9: Loss Task A: 1.1934, Loss Task B: 1.4742, Total Loss: 2.6676\n",
      "Epoch 10: Loss Task A: 1.2037, Loss Task B: 1.4981, Total Loss: 2.7017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test out the training loop\n",
    "sentences = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"Apple Inc. is based in Cupertino, California.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Elon Musk is the CEO of Tesla.\",\n",
    "    \"Microsoft Corporation is headquartered in Redmond.\",\n",
    "    \"Google was founded by Larry Page and Sergey Brin.\",\n",
    "    \"The Great Wall of China is a famous landmark.\"\n",
    "]\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = padded_tensor.shape[1]\n",
    "NUM_CLASSES_A = 3 \n",
    "NUM_CLASSES_B = 4  \n",
    "\n",
    "# Get random labels for both the tasks\n",
    "labels_a = torch.randint(0, 3, (len(sentences),))  \n",
    "labels_b = torch.randint(0, 4, (len(sentences), MAX_LEN)) \n",
    "\n",
    "vocab, padded_tensor = preprocess_sentences(sentences)\n",
    "\n",
    "# Loss functions\n",
    "lf_a = nn.CrossEntropyLoss()\n",
    "lf_b = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(multi_task_model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the model\n",
    "multi_task_model = MultiTaskTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, NUM_CLASSES_A, NUM_CLASSES_B, MAX_LEN)\n",
    "train_model(multi_task_model, optimizer, lf_a, lf_b, padded_tensor, labels_a, labels_b, NUM_CLASSES_B, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
