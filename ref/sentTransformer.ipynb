{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/muditjindal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MultiHead Self Attention Block\n",
    "class SelfMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads): \n",
    "        super(SelfMultiHeadAttention, self).__init__()\n",
    "        # Check if num_heads divides d_model \n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Size per attention head\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Initialize the Q, K and V\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size\n",
    "        batch_size = x.shape[0]\n",
    "        # Pass in the x to Q, K and V and reshape\n",
    "        # Q, K, V: (batch_size, num_heads, seq_length, d_k)\n",
    "        Q = self.q_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        atn_weights = F.softmax(scores, dim=-1) \n",
    "        atn_out = torch.matmul(atn_weights, V) # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        atn_out = atn_out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # Shape: (batch_size, seq_len, d_model)\n",
    "        return self.out_linear(atn_out) # Shape: (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHead Attention\n",
    "attn_layer = SelfMultiHeadAttention(d_model=128, num_heads=8)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128)\n",
    "# Expected (batch_size, seq_len, d_model)\n",
    "print(attn_layer(test_input).shape)\n",
    "assert test_input.shape == attn_layer(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Class\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test FeedForward\n",
    "ff_layer = FeedForward(d_model=128, d_ff=512)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128)\n",
    "# Expected: (batch_size, seq_len, d_model)\n",
    "print(ff_layer(test_input).shape)\n",
    "assert test_input.shape == ff_layer(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (sin & cosine curves)\n",
    "class PositionalEncode(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncode, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Encode even indices (sin curve)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Encode the odd indices (cos curve)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_len, d_model) (to handle with batch_size)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test PositionalEncoding\n",
    "pos_enc = PositionalEncode(d_model=128, max_len=50)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128) \n",
    "# Expected: (batch_size, seq_len, d_model)\n",
    "print(pos_enc(test_input).shape)\n",
    "assert test_input.shape == pos_enc(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder class\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.2):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_atn = SelfMultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atn_out = self.self_atn(x)\n",
    "        x = self.norm1(x + self.dropout(atn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test TransformerEncoderLayer\n",
    "encoder_layer = TransformerEncoder(d_model=128, num_heads=8, d_ff=512)\n",
    "# (batch_size, seq_len, d_model)\n",
    "test_input = torch.randn(2, 50, 128)\n",
    "# Expected: (batch_size, seq_len, d_model)\n",
    "print(encoder_layer(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer class\n",
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=100):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encode = PositionalEncode(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encode(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        x = x.permute(0, 2, 1) # Shape: (batch_size, d_model, seq_len)\n",
    "        x = self.pooling(x).squeeze(-1) # Shape (batch_size, d_model, 1) => (batch_size, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test SentenceTransformer\n",
    "VOCAB_SIZE = 10000\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = 50\n",
    "\n",
    "model = SentenceTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, MAX_LEN)\n",
    "# (batch_size, seq_len)\n",
    "test_input = torch.randint(0, VOCAB_SIZE, (2, MAX_LEN))\n",
    "# Expected: (batch_size, d_model)\n",
    "print(model(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 795648\n"
     ]
    }
   ],
   "source": [
    "# function to count the number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Trainable Parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to preprocess sentences. Gets vocab and the padded sentence\n",
    "def preprocess_sentences(sentences):\n",
    "    # Create a vocab\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    vocab['<PAD>'] = 0\n",
    "\n",
    "    # Tokenize and convert to IDs\n",
    "    tokenized = [word_tokenize(s.lower()) for s in sentences]\n",
    "    print(tokenized)\n",
    "    indexed = [[vocab[w] for w in s] for s in tokenized]\n",
    "    print(indexed)\n",
    "\n",
    "    # Padding\n",
    "    max_len = max(len(s) for s in indexed)\n",
    "    # Add padding to shorter sentences\n",
    "    padded = [s + [0] * (max_len - len(s)) for s in indexed] \n",
    "\n",
    "    # Convert to tensor\n",
    "    padded_tensor = torch.tensor(padded)\n",
    "\n",
    "    return vocab, padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'cat', 'sat', 'on', 'the', 'mat', '.'], ['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'], ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', '.']]\n",
      "[[1, 2, 3, 4, 1, 5, 6], [7, 8, 9, 10, 11, 12, 1, 13, 14, 6], [15, 16, 17, 18, 1, 19, 6]]\n"
     ]
    }
   ],
   "source": [
    "vocab, padded_tensor = preprocess_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  1,  5,  6,  0,  0,  0],\n",
       "        [ 7,  8,  9, 10, 11, 12,  1, 13, 14,  6],\n",
       "        [15, 16, 17, 18,  1, 19,  6,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.preprocess_sentences.<locals>.<lambda>()>,\n",
       "            {'<PAD>': 0,\n",
       "             'the': 1,\n",
       "             'cat': 2,\n",
       "             'sat': 3,\n",
       "             'on': 4,\n",
       "             'mat': 5,\n",
       "             '.': 6,\n",
       "             'a': 7,\n",
       "             'quick': 8,\n",
       "             'brown': 9,\n",
       "             'fox': 10,\n",
       "             'jumps': 11,\n",
       "             'over': 12,\n",
       "             'lazy': 13,\n",
       "             'dog': 14,\n",
       "             'artificial': 15,\n",
       "             'intelligence': 16,\n",
       "             'is': 17,\n",
       "             'transforming': 18,\n",
       "             'world': 19})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tensor.shape # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 128])\n",
      "Embeddings: tensor([[ 4.6674e-01, -4.6863e-01, -4.7155e-01, -5.4400e-01,  1.2605e-01,\n",
      "         -6.3041e-01, -1.1903e+00, -3.0121e-01,  9.0766e-02,  4.1755e-01,\n",
      "         -3.9235e-01, -1.0099e+00,  2.0774e-02, -9.6504e-01, -8.8842e-01,\n",
      "         -1.7547e-01,  6.5008e-01, -1.0690e-01,  2.6159e-01,  2.6579e-01,\n",
      "          6.8265e-01, -9.6889e-02, -7.0535e-01, -6.8285e-01, -3.2448e-01,\n",
      "          3.3838e-01,  1.3971e-01, -2.9932e-01,  6.8125e-01,  4.6388e-01,\n",
      "         -4.2941e-02,  4.6098e-01, -5.3407e-02,  6.8306e-01,  5.9274e-02,\n",
      "         -2.0512e-01, -2.3387e-01,  7.7714e-01, -2.3095e-01, -4.5909e-01,\n",
      "          1.2471e-01,  4.8962e-01,  3.6096e-01,  1.0197e+00,  3.2431e-01,\n",
      "          3.9017e-02,  2.7602e-01, -2.1988e-01,  1.1412e-01,  6.8797e-01,\n",
      "         -2.8035e-01,  8.1698e-01,  4.2874e-01,  4.3071e-01,  1.3378e-01,\n",
      "          1.5567e-01, -3.1157e-01, -3.7306e-01, -9.5627e-02,  1.1642e-01,\n",
      "         -2.9743e-01,  1.6928e-01,  1.0810e+00,  3.9694e-01, -8.6499e-01,\n",
      "         -3.9450e-01,  3.8814e-01, -4.3711e-01,  6.4215e-01, -7.5904e-01,\n",
      "         -2.6868e-01,  7.2320e-01, -1.1833e+00,  1.0815e+00,  1.4191e-02,\n",
      "          6.9396e-01,  2.4670e-02, -1.8158e-01, -3.6214e-01,  3.4822e-01,\n",
      "         -9.4120e-02, -5.3342e-01, -5.8378e-01, -7.9897e-01,  1.3641e+00,\n",
      "          1.4883e-01, -1.5607e-01, -3.9597e-01, -1.9331e+00,  1.0417e+00,\n",
      "         -1.2960e-01, -4.8047e-01, -2.3553e-01,  4.3605e-01, -1.0414e+00,\n",
      "          9.1392e-01, -9.3702e-01,  2.5269e-01, -5.6239e-01, -2.5489e-01,\n",
      "         -8.0784e-01,  2.4455e-01,  1.4934e-02,  5.2301e-01, -4.4884e-01,\n",
      "         -1.7463e-02, -6.9181e-01,  7.2443e-01,  4.6706e-01,  4.0743e-01,\n",
      "          3.5266e-01, -7.1918e-02, -4.9169e-01,  1.9517e+00, -1.1946e+00,\n",
      "          1.1936e+00, -8.5693e-01, -6.8911e-02,  1.0331e-01,  3.6221e-01,\n",
      "         -7.2588e-01,  3.1420e-02, -5.3558e-01,  1.2488e+00,  4.6931e-02,\n",
      "          1.0029e+00, -2.3621e-01,  2.9223e-01],\n",
      "        [ 1.5135e-01, -3.3446e-01, -1.1181e+00, -3.2010e-01,  2.0476e-01,\n",
      "         -1.2403e-01, -1.0704e+00, -3.1649e-01, -1.8796e-01, -7.1405e-01,\n",
      "         -3.2433e-01, -5.2255e-01,  1.1538e-01, -5.6500e-01, -4.0888e-01,\n",
      "         -5.0358e-01, -7.0667e-02,  1.3171e-01, -4.7653e-01,  1.2583e-01,\n",
      "          7.1728e-01, -2.0546e-01, -3.5155e-01, -4.4045e-01, -3.4591e-01,\n",
      "          3.7145e-01, -2.6182e-01, -2.3572e-01,  1.1147e+00,  2.1299e-01,\n",
      "          1.3252e-01, -4.4834e-01, -6.0112e-01,  2.6564e-01,  1.4387e-01,\n",
      "         -2.0821e-01, -4.7086e-01,  7.6782e-01, -1.9998e-01,  4.6013e-02,\n",
      "          1.4208e-01,  1.1464e+00, -2.0985e-01,  7.4272e-01,  1.2237e-02,\n",
      "          1.5913e-01,  5.7291e-01,  7.3860e-02,  2.2949e-01,  6.6596e-01,\n",
      "         -2.4246e-01,  8.8216e-01,  6.7095e-01,  4.2697e-01,  3.7112e-01,\n",
      "          1.8598e-01, -2.5377e-01,  1.9767e-02, -4.5491e-01,  2.8551e-01,\n",
      "         -1.7281e-02,  9.6767e-01,  3.8680e-01,  4.6352e-01, -7.6557e-01,\n",
      "         -7.6884e-01,  1.3000e-01, -1.6761e-01,  4.1426e-01, -6.5128e-01,\n",
      "         -1.0915e+00,  4.6556e-01, -8.0593e-01,  9.5579e-01, -2.3687e-01,\n",
      "          1.1051e+00,  3.5959e-01, -1.3083e-01, -6.2412e-01, -7.9220e-01,\n",
      "          5.3032e-02, -2.3988e-02, -3.2851e-01,  2.4008e-02,  8.3830e-01,\n",
      "          3.7386e-02,  3.7372e-01,  5.2278e-01, -1.0066e+00,  1.0222e+00,\n",
      "         -6.7582e-01, -2.3579e-01, -3.2156e-01,  5.0262e-01, -4.9538e-01,\n",
      "          1.1123e+00, -4.2656e-01,  4.9075e-01, -4.4376e-01,  1.3488e-02,\n",
      "         -8.9545e-01,  3.4601e-01, -7.3936e-01,  2.9272e-01,  2.7541e-01,\n",
      "          5.7180e-01,  1.2727e-01,  3.8838e-01,  1.0901e-02,  3.0810e-01,\n",
      "          9.0826e-02,  3.3533e-01, -4.2997e-01,  1.3669e+00, -1.5427e-01,\n",
      "          3.1086e-01, -5.1200e-01,  2.9671e-01,  3.5890e-01, -2.0472e-01,\n",
      "         -7.1076e-01,  1.1042e-01, -1.3385e+00,  1.2669e+00, -4.3911e-01,\n",
      "          3.5338e-01, -1.2126e+00,  4.9600e-01],\n",
      "        [ 3.2061e-01, -8.4986e-01, -1.0297e+00, -6.1338e-01, -7.2225e-01,\n",
      "         -4.3043e-01, -6.1008e-01, -1.2167e-01,  7.4064e-02, -6.5396e-02,\n",
      "         -3.7574e-01, -7.6334e-01,  1.5833e-01, -7.9431e-01, -4.8326e-01,\n",
      "         -9.1146e-02,  5.3554e-01, -9.0461e-02,  3.2634e-02,  3.7325e-01,\n",
      "          7.3438e-01,  2.6105e-01, -7.2182e-01,  3.6865e-02, -1.2890e+00,\n",
      "          2.4309e-01,  3.6068e-01, -5.8547e-01,  3.7621e-01,  5.7496e-01,\n",
      "         -8.6964e-02,  9.1859e-02, -5.8676e-02,  1.0013e+00,  5.6342e-01,\n",
      "         -1.3690e-01, -3.9677e-01,  1.0527e+00, -4.5422e-01, -6.5722e-02,\n",
      "          2.2885e-01,  3.3619e-01,  6.3725e-03,  1.0553e+00,  3.9676e-01,\n",
      "         -1.5747e-01,  3.4225e-01,  5.9111e-01,  3.0618e-01,  1.7931e-01,\n",
      "         -3.8334e-01,  1.1997e+00,  1.6517e-01,  3.4394e-01,  1.1801e-01,\n",
      "          3.3909e-01, -4.1409e-01,  9.4079e-02,  1.2934e-01, -1.2987e-03,\n",
      "         -6.7519e-02,  1.1235e-01,  1.9729e-01,  1.3671e-01, -1.1165e+00,\n",
      "         -5.7870e-01,  5.5925e-02, -1.9990e-01,  9.5795e-01, -1.6971e-01,\n",
      "          3.6243e-02,  6.3327e-01, -8.1473e-01,  1.3540e+00,  3.5049e-01,\n",
      "          5.5016e-01,  3.4007e-01,  4.1560e-01,  1.3579e-01,  2.0670e-02,\n",
      "          6.0114e-02, -1.2328e-01,  2.8925e-01, -4.1607e-01,  1.4287e+00,\n",
      "          4.3714e-01, -4.2516e-01, -5.3785e-01, -1.7524e+00, -2.0772e-01,\n",
      "          2.5850e-01, -6.6893e-01, -2.1349e-01,  6.7387e-01, -1.1463e+00,\n",
      "          6.0170e-01, -8.5430e-01,  3.4767e-01, -7.9608e-01, -3.7665e-01,\n",
      "         -6.6483e-02,  1.7036e-01, -4.8079e-01, -1.8936e-01, -3.4414e-01,\n",
      "          3.7290e-01, -6.3589e-01,  1.0109e+00, -3.1419e-01,  4.2896e-02,\n",
      "          4.1920e-01, -9.0507e-01, -5.2429e-01,  1.2520e+00, -9.7869e-01,\n",
      "          1.0411e+00, -6.8600e-01,  1.8380e-01, -6.5764e-03,  5.4328e-01,\n",
      "         -6.4065e-01,  1.0472e-01, -1.9884e-02,  6.3782e-01,  2.8951e-03,\n",
      "          8.3583e-01, -7.5467e-01,  1.7098e-01]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128 # Dimension of the fixed size sentence embedding\n",
    "NUM_HEADS = 8 # Number of heads for multi head attention \n",
    "NUM_LAYERS = 4 # Number of repeated multiheadattention blocks\n",
    "D_FF = 512 # intermediate dimension for feedforward network \n",
    "model = SentenceTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, max_len=len(padded_tensor[0]))\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = model(padded_tensor)\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # Expected shape: (batch_size, D_MODEL)\n",
    "print(\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Multi-Task Learning Expansion\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, num_classes_a, num_classes_b, max_len=100):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encode = PositionalEncode(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Task A: Sentence Classification Head\n",
    "        self.classifier_a = nn.Linear(d_model, num_classes_a)\n",
    "\n",
    "        # Task B: Named Entity Recognition (NER) - per token basis Head\n",
    "        self.ner_classifier_b = nn.Linear(d_model, num_classes_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> Shape: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encode(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x) # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Task A: Sentence Classification\n",
    "        # we want to pool over the seq_len \n",
    "        x_pooled = x.permute(0, 2, 1) # Shape: (batch_size, d_model, seq_len)\n",
    "        x_pooled = self.pooling(x_pooled).squeeze(-1) # Shape: (batch_size, d_model)\n",
    "        out_task_a = self.classifier_a(x_pooled) # Shape: (batch_size, num_classes_a)\n",
    "        # Task B: NER \n",
    "        # We pass in x and not x_pooled as we want per token label in NER\n",
    "        out_task_b = self.ner_classifier_b(x) # Shape: (batch_size, seq_len, num_classes_b)\n",
    "\n",
    "        return out_task_a, out_task_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification: torch.Size([2, 3])\n",
      "NER: torch.Size([2, 50, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test Multi Task Transformer\n",
    "# Model Initialization\n",
    "VOCAB_SIZE = 10000\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = 50\n",
    "NUM_CLASSES_A = 3 \n",
    "NUM_CLASSES_B = 4  \n",
    "\n",
    "multi_task_model = MultiTaskTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, NUM_CLASSES_A, NUM_CLASSES_B, MAX_LEN)\n",
    "\n",
    "test_input = torch.randint(0, VOCAB_SIZE, (2, MAX_LEN)) # (batch_size, seq_len)\n",
    "out_task_a, out_task_b = multi_task_model(test_input)\n",
    "print(f\"Sentence Classification: {out_task_a.shape}\") # Expected: (batch_size, num_classes_a)\n",
    "print(f\"NER: {out_task_b.shape}\") # Expected: (batch_size, seq_len, num_classes_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification output: tensor([[ 0.0507,  0.0416,  0.4814],\n",
      "        [ 0.0211, -0.0633,  0.4029]], grad_fn=<AddmmBackward0>)\n",
      "NER output: tensor([[[ 1.1841e+00, -7.1495e-01,  4.8525e-02,  4.7646e-02],\n",
      "         [ 5.4922e-01,  9.4005e-02,  2.2591e-01, -6.1908e-02],\n",
      "         [ 1.4539e-01,  1.2950e+00,  3.8574e-01, -5.3482e-02],\n",
      "         [ 8.1093e-01,  4.4390e-01, -4.9584e-01, -2.8783e-01],\n",
      "         [ 7.4695e-01,  1.0204e+00,  3.4611e-01,  6.7273e-01],\n",
      "         [ 1.3796e-01,  9.7461e-01,  2.8913e-01,  7.6818e-01],\n",
      "         [ 1.0523e+00,  1.0082e-01, -5.5909e-01, -9.7521e-01],\n",
      "         [ 7.8043e-01,  2.6486e-02,  2.0210e-02, -3.7988e-01],\n",
      "         [ 7.7239e-01, -8.7847e-01, -5.0499e-01,  9.2409e-01],\n",
      "         [ 3.5308e-01, -1.0130e-01,  3.8011e-01,  9.8022e-02],\n",
      "         [ 4.5162e-01,  7.6195e-01,  8.0909e-01,  1.0101e+00],\n",
      "         [ 9.8108e-01, -2.8731e-01, -5.2479e-01,  3.0944e-01],\n",
      "         [-2.7185e-01, -3.7539e-01,  1.2449e-02, -1.1229e-01],\n",
      "         [ 7.6041e-01,  4.6045e-01, -1.3892e-02,  1.4374e-01],\n",
      "         [ 5.3881e-01,  3.9088e-01,  4.2117e-01, -2.7233e-01],\n",
      "         [ 5.7802e-02,  6.8310e-01, -6.0630e-02, -3.5230e-01],\n",
      "         [ 1.0521e-02,  1.9117e-01, -3.0387e-01, -2.3462e-01],\n",
      "         [-6.9883e-02, -5.2092e-01,  3.3382e-01,  6.5678e-02],\n",
      "         [-7.8900e-01,  6.0496e-01,  4.4762e-01,  8.2207e-01],\n",
      "         [-2.9758e-01,  6.6629e-01,  6.0451e-01,  1.0662e-01],\n",
      "         [ 2.4023e-01,  3.1528e-02,  8.2318e-01,  5.0933e-01],\n",
      "         [ 3.6388e-01,  1.6430e+00,  1.0145e+00, -5.7986e-02],\n",
      "         [ 2.6261e-01,  2.8037e-02, -4.5572e-01, -2.9414e-01],\n",
      "         [ 1.2812e-01,  1.2146e+00,  5.2950e-01, -1.3664e-01],\n",
      "         [ 8.3233e-01,  4.2575e-01,  3.7748e-01, -1.3812e-02],\n",
      "         [-4.2596e-02,  2.8478e-01,  1.3600e-01,  2.9650e-01],\n",
      "         [ 8.4366e-01,  1.9090e-01, -3.1710e-01, -8.3881e-02],\n",
      "         [ 9.3871e-01,  7.1929e-02,  4.1072e-01,  7.3170e-01],\n",
      "         [ 5.8550e-01,  8.2547e-01, -4.0498e-01,  6.2057e-01],\n",
      "         [ 1.1647e-01,  1.0156e+00,  2.3019e-01, -1.3383e-01],\n",
      "         [ 5.1034e-01,  2.2492e+00, -1.3907e-01,  4.4347e-01],\n",
      "         [ 4.4980e-01,  7.2128e-01, -5.6424e-01,  4.6896e-01],\n",
      "         [ 4.2983e-01, -2.5312e-01,  6.6905e-01,  5.0738e-01],\n",
      "         [ 1.3312e-02,  1.0046e+00, -3.0641e-01,  5.2246e-01],\n",
      "         [-2.7870e-01,  6.1286e-01,  2.1703e-02, -1.7973e-01],\n",
      "         [-3.6230e-01, -2.5244e-01, -3.8707e-01, -3.9000e-01],\n",
      "         [ 6.3760e-01,  6.7363e-01,  2.0863e-01, -8.8130e-02],\n",
      "         [-9.0497e-02,  1.0844e+00,  2.7966e-01,  6.8694e-01],\n",
      "         [-1.2089e-01,  1.1811e+00,  1.5141e-01,  1.5297e-01],\n",
      "         [ 8.4713e-01,  4.0847e-01,  4.8109e-01,  4.8080e-01],\n",
      "         [-2.7033e-02, -6.3139e-03,  8.6737e-01,  9.0017e-01],\n",
      "         [-1.4573e-01,  5.1426e-01,  5.6034e-01,  9.9722e-01],\n",
      "         [ 4.5725e-01,  5.7987e-01,  6.7792e-01, -1.6634e-01],\n",
      "         [ 1.6899e-01,  4.4586e-01,  1.7748e-01,  6.1099e-02],\n",
      "         [-3.7743e-01, -4.0622e-01,  6.4149e-02,  4.0411e-01],\n",
      "         [ 1.1978e+00, -1.4953e-01, -5.7474e-01,  1.0957e+00],\n",
      "         [ 4.4598e-01,  1.9849e-01, -5.3644e-06,  1.3400e-01],\n",
      "         [ 8.2871e-02,  5.0151e-01,  4.4030e-01,  7.0451e-02],\n",
      "         [ 5.4754e-01, -6.6721e-01,  1.1982e+00, -1.3828e-01],\n",
      "         [ 7.5834e-02, -1.9557e-01, -2.3254e-02, -1.5362e-01]],\n",
      "\n",
      "        [[ 4.9595e-02,  1.5446e-01, -1.1245e-01,  5.9561e-01],\n",
      "         [ 3.1158e-01, -7.2198e-02,  4.7828e-01,  4.8149e-01],\n",
      "         [ 1.7861e-01,  5.4629e-01,  3.2219e-01,  4.6050e-01],\n",
      "         [ 4.6165e-01, -1.9819e-01,  1.3421e-01, -1.4430e-01],\n",
      "         [ 8.6107e-01,  3.3163e-01,  2.4053e-01,  1.2688e-01],\n",
      "         [ 8.4856e-01, -1.3552e-01,  8.2880e-02,  7.8671e-01],\n",
      "         [ 7.2709e-01, -4.6602e-01,  9.4963e-01, -2.7810e-02],\n",
      "         [ 3.5175e-01,  1.7567e-01,  9.0398e-01,  5.3608e-01],\n",
      "         [-3.1468e-01,  3.4698e-01,  3.0299e-01, -1.6419e-01],\n",
      "         [ 1.4799e-01, -1.8122e-01,  3.7738e-01, -6.2392e-01],\n",
      "         [ 8.2874e-01,  6.2549e-01, -5.5613e-01, -1.0114e+00],\n",
      "         [ 6.4047e-01,  5.5320e-01, -2.6811e-01,  5.0416e-01],\n",
      "         [ 1.1274e-01,  3.8513e-01, -3.9971e-01, -3.5849e-02],\n",
      "         [ 2.0716e-01,  1.8157e-01, -3.5674e-01, -7.0010e-01],\n",
      "         [ 5.1312e-01,  3.4830e-01, -6.2940e-02,  1.1139e-01],\n",
      "         [ 1.0763e+00,  4.9311e-01, -2.0243e-01, -4.0011e-02],\n",
      "         [-3.3585e-01,  9.7947e-01, -2.8073e-01, -4.4370e-01],\n",
      "         [ 3.7423e-01,  6.6155e-01,  1.1097e-01, -5.3813e-01],\n",
      "         [ 2.5627e-01,  1.3182e+00,  3.9846e-01,  1.7115e-01],\n",
      "         [-1.5176e-01, -2.8728e-01,  5.1271e-01,  1.1445e-01],\n",
      "         [-4.8801e-02,  1.7205e+00, -2.9861e-01,  5.1711e-01],\n",
      "         [-5.9306e-01,  2.3329e-01, -1.8198e-01,  4.2072e-01],\n",
      "         [ 6.0225e-02,  4.5432e-01,  1.5762e-01, -4.9481e-01],\n",
      "         [-8.5678e-01,  1.0135e+00,  4.2218e-01,  4.3087e-01],\n",
      "         [ 4.8423e-01,  5.4769e-01,  8.5824e-01, -1.7488e-01],\n",
      "         [-2.2419e-02,  1.5412e-02,  9.3650e-01,  1.2412e-01],\n",
      "         [ 5.6594e-01,  1.1264e+00, -6.5943e-01, -2.9143e-01],\n",
      "         [ 6.6800e-01,  6.0601e-01, -1.1571e-01,  1.7338e-01],\n",
      "         [-7.1831e-01,  8.6909e-01,  4.2670e-01,  3.4464e-01],\n",
      "         [ 8.1612e-01,  2.0652e-01,  9.3815e-01, -3.0806e-01],\n",
      "         [ 7.4690e-01,  5.4821e-01, -1.2237e-01, -3.7405e-01],\n",
      "         [ 3.3735e-01,  7.3466e-01, -4.1026e-01, -5.3356e-01],\n",
      "         [-8.2013e-02,  5.4668e-01, -2.2229e-01,  3.6112e-02],\n",
      "         [-9.9036e-01,  2.8677e-01, -7.6131e-01,  1.8955e-01],\n",
      "         [ 3.5483e-01,  1.3599e-01, -2.4828e-01,  2.2702e-01],\n",
      "         [ 1.2561e+00,  7.7104e-01, -6.8102e-01, -2.5955e-01],\n",
      "         [-5.8845e-01, -3.5706e-01, -3.1377e-01, -8.8519e-01],\n",
      "         [ 1.6949e-01,  9.3599e-01,  2.2946e-01, -1.6488e-01],\n",
      "         [ 6.0217e-01,  5.5441e-02,  5.8120e-01,  2.2534e-01],\n",
      "         [-1.0707e-01, -2.4684e-01, -1.9244e-01,  1.0821e+00],\n",
      "         [ 4.3644e-01, -3.0220e-01,  2.5132e-01,  2.3537e-01],\n",
      "         [ 4.4176e-01, -1.6268e-01,  8.8757e-01,  2.5536e-01],\n",
      "         [-1.6193e-01, -9.6262e-02,  1.0195e+00,  2.6081e-01],\n",
      "         [-6.1400e-01,  7.3599e-01,  8.0867e-01,  1.8800e-01],\n",
      "         [ 3.7255e-02,  7.6188e-01,  3.8255e-01,  9.0302e-01],\n",
      "         [ 3.3191e-01, -1.6765e-01,  6.4591e-01,  3.5720e-01],\n",
      "         [-4.5915e-01,  1.0182e+00, -8.5854e-01,  9.7018e-01],\n",
      "         [-4.9604e-01,  6.4798e-02, -9.4369e-01,  2.4456e-01],\n",
      "         [ 1.9346e-01,  5.0406e-01, -3.1101e-01, -2.4326e-01],\n",
      "         [ 3.8264e-02, -4.7004e-01, -3.6493e-01,  2.0324e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence Classification output: {out_task_a}\")\n",
    "print(f\"NER output: {out_task_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification Prediction: [2, 2]\n",
      "NER Prediction: [[0, 0, 1, 0, 1, 1, 0, 0, 3, 2, 3, 0, 2, 0, 0, 1, 1, 2, 3, 1, 2, 1, 0, 1, 0, 3, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 3, 3, 2, 1, 3, 0, 0, 1, 2, 0], [3, 3, 1, 0, 0, 0, 2, 2, 1, 2, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 1, 3, 1, 1, 2, 2, 1, 0, 1, 2, 0, 1, 1, 1, 0, 0, 2, 1, 0, 3, 0, 2, 2, 2, 3, 2, 1, 3, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Classification Prediction:\", torch.argmax(out_task_a, dim=1).tolist())\n",
    "print(\"NER Prediction:\", torch.argmax(out_task_b, dim=2).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barack', 'obama', 'was', 'the', '44th', 'president', 'of', 'the', 'united', 'states', '.'], ['apple', 'inc.', 'is', 'based', 'in', 'cupertino', ',', 'california', '.']]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 4, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 10]]\n",
      "Sentence Classification: torch.Size([2, 3])\n",
      "NER: torch.Size([2, 11, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test out on sentences\n",
    "sentences = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"Apple Inc. is based in Cupertino, California.\"\n",
    "]\n",
    "\n",
    "vocab, padded_tensor = preprocess_sentences(sentences)\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = len(padded_tensor[0])\n",
    "NUM_CLASSES_A = 3 \n",
    "NUM_CLASSES_B = 4  \n",
    "\n",
    "multi_task_model = MultiTaskTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, NUM_CLASSES_A, NUM_CLASSES_B, MAX_LEN)\n",
    "out_task_a, out_task_b = multi_task_model(padded_tensor)\n",
    "print(f\"Sentence Classification: {out_task_a.shape}\") # Expected: (batch_size, num_classes_a)\n",
    "print(f\"NER: {out_task_b.shape}\") # Expected: (batch_size, seq_len, num_classes_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification Prediction: [0, 1]\n",
      "NER Prediction: [[2, 2, 3, 2, 1, 3, 3, 2, 1, 2, 1], [0, 0, 3, 1, 0, 3, 0, 1, 1, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Classification Prediction:\", torch.argmax(out_task_a, dim=1).tolist())\n",
    "print(\"NER Prediction:\", torch.argmax(out_task_b, dim=2).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop function\n",
    "def train_model(model, optimizer, lf_a, lf_b, inputs, labels_a, labels_b, num_classes_b, epochs=3):\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the outputs from the model\n",
    "        out_task_a, out_task_b = model(inputs)\n",
    "\n",
    "        # Calculate the loss for both the tasks\n",
    "        loss_a = lf_a(out_task_a, labels_a)\n",
    "        loss_b = lf_b(out_task_b.view(-1, num_classes_b), labels_b.view(-1))\n",
    "        \n",
    "        # Get the total loss\n",
    "        total_loss = loss_a + loss_b\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss Task A: {loss_a.item():.4f}, Loss Task B: {loss_b.item():.4f}, Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barack', 'obama', 'was', 'the', '44th', 'president', 'of', 'the', 'united', 'states', '.'], ['apple', 'inc.', 'is', 'based', 'in', 'cupertino', ',', 'california', '.'], ['the', 'eiffel', 'tower', 'is', 'located', 'in', 'paris', '.'], ['elon', 'musk', 'is', 'the', 'ceo', 'of', 'tesla', '.'], ['microsoft', 'corporation', 'is', 'headquartered', 'in', 'redmond', '.'], ['google', 'was', 'founded', 'by', 'larry', 'page', 'and', 'sergey', 'brin', '.'], ['the', 'great', 'wall', 'of', 'china', 'is', 'a', 'famous', 'landmark', '.']]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 4, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 10], [4, 19, 20, 13, 21, 15, 22, 10], [23, 24, 13, 4, 25, 7, 26, 10], [27, 28, 13, 29, 15, 30, 10], [31, 3, 32, 33, 34, 35, 36, 37, 38, 10], [4, 39, 40, 7, 41, 13, 42, 43, 44, 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:00<00:00, 79.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss Task A: 1.1526, Loss Task B: 1.4956, Total Loss: 2.6481\n",
      "Epoch 2: Loss Task A: 1.1794, Loss Task B: 1.4776, Total Loss: 2.6571\n",
      "Epoch 3: Loss Task A: 1.1631, Loss Task B: 1.4883, Total Loss: 2.6513\n",
      "Epoch 4: Loss Task A: 1.1579, Loss Task B: 1.4805, Total Loss: 2.6385\n",
      "Epoch 5: Loss Task A: 1.1877, Loss Task B: 1.4545, Total Loss: 2.6422\n",
      "Epoch 6: Loss Task A: 1.1773, Loss Task B: 1.4739, Total Loss: 2.6512\n",
      "Epoch 7: Loss Task A: 1.1503, Loss Task B: 1.4629, Total Loss: 2.6132\n",
      "Epoch 8: Loss Task A: 1.2007, Loss Task B: 1.4954, Total Loss: 2.6961\n",
      "Epoch 9: Loss Task A: 1.1934, Loss Task B: 1.4742, Total Loss: 2.6676\n",
      "Epoch 10: Loss Task A: 1.2037, Loss Task B: 1.4981, Total Loss: 2.7017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test out the training loop\n",
    "sentences = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"Apple Inc. is based in Cupertino, California.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Elon Musk is the CEO of Tesla.\",\n",
    "    \"Microsoft Corporation is headquartered in Redmond.\",\n",
    "    \"Google was founded by Larry Page and Sergey Brin.\",\n",
    "    \"The Great Wall of China is a famous landmark.\"\n",
    "]\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = padded_tensor.shape[1]\n",
    "NUM_CLASSES_A = 3 \n",
    "NUM_CLASSES_B = 4  \n",
    "\n",
    "# Get random labels for both the tasks\n",
    "labels_a = torch.randint(0, 3, (len(sentences),))  \n",
    "labels_b = torch.randint(0, 4, (len(sentences), MAX_LEN)) \n",
    "\n",
    "vocab, padded_tensor = preprocess_sentences(sentences)\n",
    "\n",
    "# Loss functions\n",
    "lf_a = nn.CrossEntropyLoss()\n",
    "lf_b = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(multi_task_model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the model\n",
    "multi_task_model = MultiTaskTransformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, NUM_CLASSES_A, NUM_CLASSES_B, MAX_LEN)\n",
    "train_model(multi_task_model, optimizer, lf_a, lf_b, padded_tensor, labels_a, labels_b, NUM_CLASSES_B, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
